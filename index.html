<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="UTF-8">
  <title>CodePen - Bootcamp_Tribute_Page_w/Test Suite </title>
  <link rel="stylesheet" href="./style.css">

</head>
<body>
<!-- partial:index.partial.html -->
<!doctype html>
<html class="no-js" lang="en">
 
<head>
  <meta charset="utf-8">
  <title>A Brief History of Neural Networks</title>
  <meta name="description" content="Daniel Danios' Tribute Page for Tech Elevator Bootcamp Prework">
  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
  
<body>
  <canvas id="connections"></canvas>
<main id="main">
  <div class="wrapper">
    <div class="title-animation">
  <h1 id="title">A Brief History of Neural Networks</h1>
  </div>
  </div>
  <p1>From modeling neuron 'connectionism' to deep learning and beyond...</p1>

  <figure id="img-div">
    <img
      id="image"
      src="https://upload.wikimedia.org/wikipedia/commons/d/d2/Internet_map_1024.jpg"
      alt="Partial map of the Internet based on the January 15, 2005 data found on opte.org."
      />
    <figcaption id="img-caption">
    Partial map of the Internet based on the January 15, 2005 data found on
    <a id="map-link"
       href="https://www.opte.org/maps/"
       target="_blank"
    >opte.org</a>. Each line is drawn between two nodes, representing two IP addresses.
    The length of the lines are indicative of the delay between those two nodes. This graph represents less than 30% of the Class C networks reachable by the data collection program in early 2005.
    </figcaption>
  </figure>

 <section id="tribute-info">
   <h3 id="headline">Concise Overview of the History of Neural Networks:</h3>
   <ul>
     <p><strong>1940s</strong></p>
     <li>
     Warren McCulloh and Walter Pitts penned a paper on how neurons might behave, later modeling a neural network with electrical circuits.
     </li>
     <li>
       Donald Hebb wrote, The Organization of Behavior, proposing the argument that neural pathways are strengthened through successive use.</li>
     <p><strong>1950s</strong></p>
     <li>
       Nathaniel Rochester manned the first effort to simulate a neural network at IBMs' research division.
     </li>
     <li>
       Frank Rosenblatt (a research psychologist working at Cornell Aeronautical Laboratory) developed Perception: a single layer of neurons able to classify pictures of a few hundred pixels. He went on to implement an algorithm that would train the system through a dataset.
     </li>
     <li>
       Brenard Widrow and Marcian Hoff developed the models, Adaline and Madaline, at Stanford. The latter was the first neural network with real-world applications.
     </li>
     <p><strong>1970s</strong></p>
     <li>
      Marvin Minsky and Seymour Papert published Perceptions: An introduction to computational geometry, in 1969, exposing the limited capabilities of the Perceptron. Coupled with the misplaced fear and unfulfilled claims led to the first 'neural network winter'.
     </li>
     <li>
        Paul Webros proposed the use of backpropagation to optimize neural networks, in his 1974 PhD thesis. However, this concept went unnoticed until the 1980s.
     </li>
     <p><strong>1980s</strong></p>
     <li>
In 1986, Rumelhart et al. popularized the use of backpropagation as a training technique for neural networks.
     </li>
     <li>
      Researchers went on to use a technique of handwriting analysis, proposed by Yann LeCun, to develop a network that could interpret handwritten digits.
     </li>
     <p><strong>1990s</strong></p>
     <li>
    Through compounding slow optimization, resulting from deeper (more stacked) networks and limited activation functionsâ€™ derivatives, the second neural network winter took over until the mid-2000s.
     </li>
     <p><strong>2000s</strong></p>
     <li>
      Through the adoption of a specific activation function (ReLu) better optimization algorithms, such as ADAM, help train neural networks more efficiently.
     </li>
     <li>
      Later in <strong>2012</strong>, at the ImageNet Large Scale Visual Recognition Challenge, a team led by Alex Krizhevsky utilized convolutional neural networks to achieve an unprecedented error rate of 15.3%.
     </li>
     <li>
      In <strong>2016</strong>, AlphaGo (DeepMind), beat the current world Go champion, a result many experts though not possible at least for another decade. It was able to process 10^170 possible games.
     </li>
     <li>
       In <strong>2020</strong>, Open AI unveiled the third generation of GPT3, an AI specializing in text production. Capable of solving equations, coding websites, telling stories, writing poetry, and even writing articles about itself. The team has since received a $1 billion dollar investment from Microsoft.
     </li>
   </ul>
     <h3>
      More information on neural networks and AI can be found at
        <a id="tribute-link" href="https://www.ibm.com/cloud/learn/neural-networks#:~:text=Neural%20networks%2C%20also%20known%20as,neurons%20signal%20to%20one%20another." target="_blank">
           this page</a> from IBM.
     </h3>
</main>
</body>
</html>
<!-- partial -->
  <script src='https://cdn.freecodecamp.org/testable-projects-fcc/v1/bundle.js'></script><script  src="./script.js"></script>

</body>
</html>
